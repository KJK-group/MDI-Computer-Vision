{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirSimDataset - Resnet50 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torchinfo\n",
    "import json\n",
    "from torchvision.models.segmentation import fcn\n",
    "import glob\n",
    "from PIL import Image\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import airsim \n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device chosen GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Device chosen GPU:\", torch.cuda.get_device_name(device))\n",
    "\n",
    "# Parameters\n",
    "num_classes = 3\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "learning_rate = 0.0001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "log_directory = f\"runs/AirSimDataset/resnet50/v0.0.0 Adam lr = {learning_rate}, epochs = {num_epochs}, batchsize ={batch_size}\"\n",
    "writer  = SummaryWriter(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def pil_to_np(img):\n",
    "    return np.array(img) \n",
    "\n",
    "# The Cityscapes dataset is avaliable in PyTorch\n",
    "# train_dataset = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='train', mode='fine', target_type='semantic', transform=transform, target_transform=pil_to_np)\n",
    "# #test_dataset  = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='test',  mode='fine', target_type='semantic', transform=pil_to_tensor, target_transform=transforms.ToTensor())\n",
    "# val_dataset   = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='val',   mode='fine', target_type='semantic', transform=transform, target_transform=pil_to_np)\n",
    "\n",
    "# DATA_DIRECTORY = \"AirSimDataset/\"\n",
    "# airSimDataset = np.load(DATA_DIRECTORY)\n",
    "# print(airSimDataset)\n",
    "\n",
    "# Help with glob from https://stackoverflow.com/questions/39195113/how-to-load-multiple-images-in-a-numpy-array\n",
    "file_list_images  = sorted(glob.glob('AirSimDataset4/img_rgbd_camera_0_*.png'))\n",
    "file_list_depths  = sorted(glob.glob('AirSimDataset4/img_rgbd_camera_1_*.pfm'))\n",
    "file_list_targets = sorted(glob.glob('AirSimDataset4/img_rgbd_camera_5_*.png'))\n",
    "\n",
    "dataset_images  = np.array([np.array(Image.open(filename))                for filename in file_list_images])\n",
    "dataset_depths  = np.array([np.array(airsim.utils.read_pfm(filename)[0])  for filename in file_list_depths])\n",
    "dataset_targets = np.array([np.array(Image.open(filename))                for filename in file_list_targets])\n",
    "\n",
    "# print(\"Images  shape:\", dataset_images.shape)\n",
    "# print(\"Targets shape:\", dataset_targets.shape)\n",
    "\n",
    "images = dataset_images[:,:,:,0:3]\n",
    "targets_RGB = dataset_targets[:,:,:,0:3]\n",
    "\n",
    "# print(\"Images cropped shape:\", images.shape)\n",
    "# print(\"Targets cropped shape:\", targets_RGB.shape)\n",
    "\n",
    "# From 3 channel RGB targets into 1\n",
    "\n",
    "## CPU\n",
    "# start0 = time.perf_counter()\n",
    "# targets = np.zeros(targets_RGB.shape[:3])\n",
    "# class_colors = [\n",
    "#     #np.array([0, 0, 0]),   # new_targets are already zero\n",
    "#     np.array([81, 38, 0]),  # Toilet\n",
    "#     np.array([41, 36, 132]) # Chair\n",
    "# ]\n",
    "\n",
    "# for i, class_color in enumerate(class_colors):                  # For every class/colour \n",
    "#     indice = np.where(np.all(targets_RGB == class_color, axis=-1))   # Where the current RGB value corresponds with hte pixel in the last axis\n",
    "#     targets[indice] = i+1        # Use that pixel index to define class number from enumerate. The +1 is due to 0 being background\n",
    "# print(f\"time0: {time.perf_counter() - start0}\")\n",
    "\n",
    "## GPU\n",
    "#start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    #color0 = torch.tensor([0, 0, 0], device=device)\n",
    "    color1 = torch.tensor([81, 38, 0], device=device)\n",
    "    color2 = torch.tensor([41, 36, 132], device=device)\n",
    "    color_map = [color1, color2]\n",
    "\n",
    "    torch_targets_RGB = torch.tensor(targets_RGB, device=device)\n",
    "    # Create target on the gpu\n",
    "    targets = torch.zeros(targets_RGB.shape[:3], device=device)\n",
    "    # convert to torch tensor and put it on gpu\n",
    "    #mid = time.perf_counter()\n",
    "    #print(f\"time gpu moving: {mid - start}\")\n",
    "    for i, c in enumerate(color_map):\n",
    "        # find all pixel equal to the color\n",
    "        indices = torch.where(torch.all(torch_targets_RGB == c, dim=-1))\n",
    "        # set class\n",
    "        targets[indices] = i + 1\n",
    "    #print(f\"time: {time.perf_counter() - mid}\")\n",
    "\n",
    "    #targets = torch.unsqueeze(targets, dim=-1)\n",
    "    images = torch.tensor(images, device=device).permute(0,3,1,2) # Permutes from [84, 720, 1280, 3] to [84, 3, 720, 1280], as that's how PyTorch likes it\n",
    "\n",
    "    # Pairing the images with the labels - train to test ratio of 5 to 1\n",
    "    num_test = int(len(targets)/5) # will return 1/5 integer of the total sample size   \n",
    "    # train_data = (images[num_test:len(targets)], targets[num_test:len(targets)])\n",
    "    # test_data = (images[:num_test], targets[:num_test])\n",
    "    train_data = [(img, t) for img, t in zip(images[num_test:len(targets)], targets[num_test:len(targets)])]\n",
    "    test_data  = [(img, t) for img, t in zip(images[:num_test], targets[:num_test])]\n",
    "\n",
    "    # Dataloaders used to batch the paired images\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Splitting the training and testing datasets into smaller batches\n",
    "    # workers = 5\n",
    "    # train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)#,  num_workers=workers)#, pin_memory=True))\n",
    "    # #test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False)#, num_workers=workers)#, pin_memory=True))\n",
    "    # val_loader   = torch.utils.data.DataLoader(dataset=val_dataset,  batch_size=batch_size, shuffle=False)#, num_workers=workers)#, pin_memory=True))\n",
    "\n",
    "    #print('Train Size: ', len(train_dataset))\n",
    "    #print('Test Size : ', len(test_dataset))\n",
    "    #print('Val Size  : ', len(val_dataset))\n",
    "\n",
    "    '''Plot from dataset'''\n",
    "    # fig, ax = plt.subplots(nrows=2,ncols=2, figsize=(24, 16))\n",
    "    # ax[0][0].imshow(images[0].to('cpu').permute(1,2,0))\n",
    "    # ax[0][1].imshow(targets[0].to('cpu'))\n",
    "    # ax[1][0].imshow(images[83].to('cpu').permute(1,2,0))\n",
    "    # ax[1][1].imshow(targets[83].to('cpu'))\n",
    "\n",
    "    '''Plot from dataloader''';\n",
    "    # plot_img, plot_target = iter(train_loader).next()\n",
    "    # plot_img = plot_img[0]\n",
    "    # plot_target = plot_target[0]\n",
    "    # fig, ax = plt.subplots(ncols=2, figsize=(24, 16))\n",
    "    # ax[0].imshow(np.array(plot_img.to('cpu')).transpose(1,2,0)) # transpose(1,2,0) changes the order of the dimensions\n",
    "    # ax[1].imshow(np.array(plot_target.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all class RGB values in an image\n",
    "# img_target = targets_RGB[0] # opencv uses GBR\n",
    "# img_target_RGB = img_target\n",
    "# print(img_target.shape)\n",
    "# # np.where(np.all(img_target == np.array()))\n",
    "# h, w = img_target_RGB.shape[:2]\n",
    "# colors=[]\n",
    "# for x in range(w):\n",
    "#     for y in range(h):\n",
    "#         pixel = img_target[y,x]\n",
    "#         if str(pixel) not in colors:\n",
    "#             colors.append(str(pixel))\n",
    "\n",
    "# print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading saved model''';\n",
    "# model = fcn.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=False, pretrained_backbone=True).to(device)\n",
    "# optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=[beta1, beta2], eps=1e-08)\n",
    "\n",
    "# loaded_checkpoint = torch.load(\"UPDATE PATH\")\n",
    "\n",
    "# # for param in model.parameters():    # Freezing the startign layers\n",
    "# #     # param.requires_grad = False\n",
    "# #     param.requires_grad = False\n",
    "\n",
    "# model.load_state_dict(loaded_checkpoint[\"model_state\"])\n",
    "# optimiser.load_state_dict(loaded_checkpoint[\"optimiser_state\"])\n",
    "# epoch = loaded_checkpoint[\"epoch\"]\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading new model''';\n",
    "model = fcn.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=False, pretrained_backbone=True).to(device)\n",
    "\n",
    "# Finetuning\n",
    "# for param in model.parameters():    # Freezing/unfreezing the starting layers\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=[beta1, beta2], eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=48'>49</a>\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# Clear old gradient values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=49'>50</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()         \u001b[39m# Calculate the gradients\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=50'>51</a>\u001b[0m optimiser\u001b[39m.\u001b[39;49mstep()        \u001b[39m# Update the model's weights - seen at model.parameters()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=52'>53</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=53'>54</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=54'>55</a>\u001b[0m     \u001b[39m# Logging the train accuracy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000007?line=55'>56</a>\u001b[0m     pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)     \u001b[39m# Evaluate along the 1st dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=102'>103</a>\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=103'>104</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=104'>105</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(bias_correction2))\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=108'>109</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=109'>110</a>\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Training'''\n",
    "# Tensorboard\n",
    "#writer.add_graph(model.cpu(), val_dataset[0][0])\n",
    "#writer.close()\n",
    "\n",
    "# Doing the training now\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "steps_until_print = batch_size\n",
    "\n",
    "# stop_training = False\n",
    "# def signal_handler(sig, frame):\n",
    "#     print('\\nDetected Ctrl+C, stopping training')\n",
    "#     stop_training = True\n",
    "#     print('Saving model')\n",
    "# signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "model.train()\n",
    "print('Starting training')\n",
    "for epoch in range(num_epochs):\n",
    "    #if stop_training: break\n",
    "\n",
    "    # Check for stop - read file for boolean to stopping safely\n",
    "    with open(\"train.json\") as train_json:\n",
    "        train_dict = json.load(train_json)\n",
    "        if train_dict[\"train\"] == \"False\": break\n",
    "\n",
    "    testing_batches = iter(test_loader) # Every epoch tests the whole dataset once\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "            # Check for stop - read file for boolean to stopping safely\n",
    "        with open(\"train.json\") as train_json:\n",
    "            train_dict = json.load(train_json)\n",
    "            if train_dict[\"train\"] == \"False\": break\n",
    "\n",
    "        images = images.to(device, torch.float32)\n",
    "        targets = targets.to(device)\n",
    "        # print('images  shape:', images.shape)\n",
    "        # print('targets shape:', targets.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)['out']\n",
    "        # print(\"outputs shape:\", outputs.shape)\n",
    "      \n",
    "        loss = criterion(outputs, targets.long())\n",
    "\n",
    "        # Backward pass\n",
    "        optimiser.zero_grad()   # Clear old gradient values\n",
    "        loss.backward()         # Calculate the gradients\n",
    "        optimiser.step()        # Update the model's weights - seen at model.parameters()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Logging the train accuracy\n",
    "            pred = torch.argmax(outputs, dim=1)     # Evaluate along the 1st dimension\n",
    "            batch_pixel_accuracy = (pred == targets).sum().item()/(batch_size*pred.shape[1]*pred.shape[2])\n",
    "            writer.add_scalar('Accuracy/training', batch_pixel_accuracy, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step \n",
    "\n",
    "            # Logging the train loss\n",
    "            writer.add_scalar('Loss/training', loss.item()/steps_until_print, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step\n",
    "\n",
    "            # For every 5 batches, test one batch. (test:train data ratio is split 1:5)\n",
    "            if (i+1) % 5 == 0:  # Logging the testing loss\n",
    "                test_images, test_targets = testing_batches.next()\n",
    "                \n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device)#.squeeze(1)\n",
    "\n",
    "                model.eval()\n",
    "                test_outputs = model(test_images.float())['out']\n",
    "                model.train()\n",
    "                    \n",
    "                test_pred = torch.argmax(test_outputs, dim=1)\n",
    "\n",
    "                '''Plot test results'''\n",
    "                fig, ax = plt.subplots(ncols=3, figsize=(24, 16))\n",
    "                ax[0].imshow(test_images[0].to('cpu', torch.uint8).permute(1,2,0)) # transpose(1,2,0) changes the order of the dimensions\n",
    "                ax[1].imshow(test_targets[0].to('cpu'))\n",
    "                ax[2].imshow(test_pred[0].to('cpu').detach())\n",
    "                plt.pause(0.01)\n",
    "                '''                     '''\n",
    "\n",
    "                # print('test_images  shape:', test_images.shape)\n",
    "                # print('test_targets shape:', test_targets.shape)\n",
    "                # print('test_outputs shape:', test_outputs.shape)\n",
    "                # print('test_pred    shape:', test_pred.shape)\n",
    "\n",
    "                # writer.add_images('test/images',      test_images                  , epoch * n_total_steps + i)\n",
    "                # writer.add_images('test/targets',     test_targets.unsqueeze(dim=1), epoch * n_total_steps + i)\n",
    "                # writer.add_images('test/predictions', test_pred.unsqueeze(dim=1)   , epoch * n_total_steps + i)\n",
    "\n",
    "                # Logging the test accuracy\n",
    "                test_batch_pixel_accuracy = (test_pred == test_targets).sum().item()/(batch_size*test_pred.shape[1]*test_pred.shape[2])\n",
    "                #print('Test batch pixel accuracy', test_batch_pixel_accuracy)\n",
    "                writer.add_scalar('Accuracy/testing', test_batch_pixel_accuracy, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step \n",
    "\n",
    "                # Logging the test loss\n",
    "                test_loss = criterion(test_outputs, test_targets.long())\n",
    "                writer.add_scalar('Loss/testing', test_loss.item()/len(test_targets), epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step\n",
    "\n",
    "                #writer.add_scalars(\"Accuracy\", {\"train\": batch_pixel_accuracy, \"test\": test_batch_pixel_accuracy}, epoch * n_total_steps + i)\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
    "                \n",
    "\n",
    "    #print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
    "\n",
    "print(\"Training is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Saving the model''';\n",
    "# checkpoint = {\n",
    "#     \"epoch\": epoch,\n",
    "#     \"model_state\": model.state_dict(),\n",
    "#     \"optimiser_state\": optimiser.state_dict()\n",
    "# }\n",
    "# torch.save(checkpoint, \"checkpoint_resnet50_0.5epoch_v2_cityscapes.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting''';\n",
    "# with torch.no_grad():\n",
    "#     iterator = iter(test_loader)\n",
    "#     images, targets = next(iterator)\n",
    "#     images = images.to(device)\n",
    "\n",
    "#     model.eval().to(device)\n",
    "#     output = model(images.to(device))['out']\n",
    "#     pred = torch.argmax(output, dim=1)\n",
    "\n",
    "#     images = images.to('cpu')\n",
    "#     targets = targets.to('cpu')\n",
    "#     output = output.to('cpu')\n",
    "#     pred = pred.to('cpu')\n",
    "#     print('image: ', images.shape)\n",
    "#     print('target:', targets.shape)\n",
    "#     print('output:', output.shape)\n",
    "#     print('pred:  ', pred.shape)\n",
    "#     fig, ax = plt.subplots(ncols=3, figsize=(24, 16))\n",
    "#     ax[0].imshow(images[1].squeeze().permute(1,2,0))  # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "#     ax[1].imshow(targets[1].squeeze()) # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "#     ax[2].imshow(pred[1].squeeze())\n",
    "\n",
    "print('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.2695312, 5.2695312, 5.2695312, ..., 7.4101562, 7.3984375,\n",
       "        7.390625 ],\n",
       "       [5.2929688, 5.2929688, 5.2929688, ..., 7.4257812, 7.4179688,\n",
       "        7.4140625],\n",
       "       [5.9453125, 5.9453125, 5.3320312, ..., 7.4492188, 7.4453125,\n",
       "        7.4453125],\n",
       "       ...,\n",
       "       [1.1044922, 1.1044922, 1.1044922, ..., 1.0625   , 1.0625   ,\n",
       "        1.0625   ],\n",
       "       [1.0996094, 1.0996094, 1.0996094, ..., 1.0585938, 1.0585938,\n",
       "        1.0585938],\n",
       "       [1.0947266, 1.0947266, 1.0947266, ..., 1.0537109, 1.0537109,\n",
       "        1.0537109]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_list_depth = sorted(glob.glob('AirSimDataset4/img_rgbd_camera_1_*.png'))\n",
    "#dataset_images  = np.array([np.array(Image.open(filename)) for filename in file_list_images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb60faf6640>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_and_depths = zip(file_list_images, file_list_depths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/project_sheet03.ipynb#ch0000012?line=0'>1</a>\u001b[0m images\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[480.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [480.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [480.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [480.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [480.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [480.,   0.,   0., ...,   0.,   0.,   0.]]),\n",
       " array([1.20996094e+00, 1.71468896e+03, 3.42816797e+03, 5.14164697e+03,\n",
       "        6.85512598e+03, 8.56860547e+03, 1.02820840e+04, 1.19955625e+04,\n",
       "        1.37090420e+04, 1.54225215e+04, 1.71360000e+04], dtype=float32),\n",
       " <a list of 640 BarContainer objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dataset_depths[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
