{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cityscapes - Resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "#import torchinfo\n",
    "import json\n",
    "from torchvision.models.segmentation import fcn\n",
    "from matplotlib import cm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device chosen GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Device chosen GPU:\", torch.cuda.get_device_name(device))\n",
    "\n",
    "# Parameters\n",
    "num_classes = 35\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 12\n",
    "batch_size = 2\n",
    "learning_rate = 0.0001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "log_directory = f\"runs/Cityscapes/resnet50/v0.1.2 Adam lr = {learning_rate}, epochs = {num_epochs}, batchsize ={batch_size}\"\n",
    "writer  = SummaryWriter(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def pil_to_np(img):\n",
    "    return np.array(img) \n",
    "\n",
    "# The Cityscapes dataset is avaliable in PyTorch\n",
    "train_dataset = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='train', mode='fine', target_type='semantic', transform=transform, target_transform=pil_to_np)\n",
    "#test_dataset  = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='test',  mode='fine', target_type='semantic', transform=pil_to_tensor, target_transform=transforms.ToTensor())\n",
    "val_dataset   = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='val',   mode='fine', target_type='semantic', transform=transform, target_transform=pil_to_np)\n",
    "\n",
    "# Splitting the training and testing datasets into smaller batches\n",
    "workers = 5\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)#,  num_workers=workers)#, pin_memory=True))\n",
    "#test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False)#, num_workers=workers)#, pin_memory=True))\n",
    "val_loader   = torch.utils.data.DataLoader(dataset=val_dataset,  batch_size=batch_size, shuffle=False)#, num_workers=workers)#, pin_memory=True))\n",
    "\n",
    "'''Plot from dataset'''\n",
    "# fig, ax = plt.subplots(ncols=2, figsize=(24, 16))\n",
    "# ax[0].imshow(np.array(train_dataset[1][0]).transpose(1,2,0)) # transpose(1,2,0) changes the order of the dimensions\n",
    "# ax[1].imshow(np.array(train_dataset[1][1]))\n",
    "\n",
    "'''Plot from dataloader''';\n",
    "# plot_img, plot_target = next(iter(val_loader))\n",
    "# plot_img = plot_img[0]\n",
    "# plot_target = plot_target[0]\n",
    "# print(plot_img.shape)\n",
    "# print(plot_target.shape)\n",
    "\n",
    "# fig, ax = plt.subplots(ncols=2, figsize=(24, 16))\n",
    "# ax[0].imshow(np.array(plot_img).transpose(1,2,0)) # transpose(1,2,0) changes the order of the dimensions\n",
    "# ax[1].imshow(np.array(plot_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading saved model''';\n",
    "model = fcn.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=False, pretrained_backbone=True).to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=[beta1, beta2], eps=1e-08)\n",
    "\n",
    "loaded_checkpoint = torch.load(\"checkpoint_resnet50_0.5epoch_cityscapes.pth\")\n",
    "\n",
    "# for param in model.parameters():    # Freezing the startign layers\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model.load_state_dict(loaded_checkpoint[\"model_state\"])\n",
    "optimiser.load_state_dict(loaded_checkpoint[\"optimiser_state\"])\n",
    "epoch = loaded_checkpoint[\"epoch\"]\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  2975\n",
      "Val Size  :  500\n"
     ]
    }
   ],
   "source": [
    "'''Loading new model''';\n",
    "model = fcn.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=False, pretrained_backbone=True).to(device)\n",
    "\n",
    "#summary(model, (3, 2048, 1024), batch_size=2)\n",
    "\n",
    "#num_features = model.features     # Getting the number of features going into the fully connected layer\n",
    "\n",
    "# Finetuning\n",
    "# for param in model.parameters():    # Freezing/unfreezing the starting layers\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=[beta1, beta2], eps=1e-8)\n",
    "\n",
    "print('Train Size: ', len(train_dataset))\n",
    "#print('Test Size : ', len(test_dataset))\n",
    "print('Val Size  : ', len(val_dataset))\n",
    "#summary(model, (3, 2048, 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images  shape: torch.Size([2, 3, 1024, 2048])\n",
      "test_targets shape: torch.Size([2, 1024, 2048])\n",
      "test_outputs shape: torch.Size([2, 35, 1024, 2048])\n",
      "test_pred    shape: torch.Size([2, 1024, 2048])\n",
      "Epoch 1/12, step 5/1488, loss = 0.23867\n",
      "test_images  shape: torch.Size([2, 3, 1024, 2048])\n",
      "test_targets shape: torch.Size([2, 1024, 2048])\n",
      "test_outputs shape: torch.Size([2, 35, 1024, 2048])\n",
      "test_pred    shape: torch.Size([2, 1024, 2048])\n",
      "Epoch 1/12, step 10/1488, loss = 0.55769\n",
      "test_images  shape: torch.Size([2, 3, 1024, 2048])\n",
      "test_targets shape: torch.Size([2, 1024, 2048])\n",
      "test_outputs shape: torch.Size([2, 35, 1024, 2048])\n",
      "test_pred    shape: torch.Size([2, 1024, 2048])\n",
      "Epoch 1/12, step 15/1488, loss = 0.24323\n",
      "test_images  shape: torch.Size([2, 3, 1024, 2048])\n",
      "test_targets shape: torch.Size([2, 1024, 2048])\n",
      "test_outputs shape: torch.Size([2, 35, 1024, 2048])\n",
      "test_pred    shape: torch.Size([2, 1024, 2048])\n",
      "Epoch 1/12, step 20/1488, loss = 0.19112\n",
      "test_images  shape: torch.Size([2, 3, 1024, 2048])\n",
      "test_targets shape: torch.Size([2, 1024, 2048])\n",
      "test_outputs shape: torch.Size([2, 35, 1024, 2048])\n",
      "test_pred    shape: torch.Size([2, 1024, 2048])\n",
      "Epoch 1/12, step 25/1488, loss = 0.30290\n",
      "test_images  shape: torch.Size([2, 3, 1024, 2048])\n",
      "test_targets shape: torch.Size([2, 1024, 2048])\n",
      "test_outputs shape: torch.Size([2, 35, 1024, 2048])\n",
      "test_pred    shape: torch.Size([2, 1024, 2048])\n",
      "Epoch 1/12, step 30/1488, loss = 0.54421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb#ch0000008?line=50'>51</a>\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# Clear old gradient values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb#ch0000008?line=51'>52</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()         \u001b[39m# Calculate the gradients\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb#ch0000008?line=52'>53</a>\u001b[0m optimiser\u001b[39m.\u001b[39;49mstep()        \u001b[39m# Update the model's weights - seen at model.parameters()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb#ch0000008?line=54'>55</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb#ch0000008?line=56'>57</a>\u001b[0m     \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# Maybe replace 1 with steps_until_print\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevork/MDI-Computer-Vision/learning_sheet09.ipynb#ch0000008?line=57'>58</a>\u001b[0m         \u001b[39m# Logging the accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=93'>94</a>\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=95'>96</a>\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=96'>97</a>\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=97'>98</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m     <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m    <a href='file:///home/kevork/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py?line=99'>100</a>\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Training'''\n",
    "# Tensorboard\n",
    "#writer.add_graph(model.cpu(), val_dataset[0][0])\n",
    "#writer.close()\n",
    "\n",
    "# Doing the training now\n",
    "\n",
    "running_loss = 0\n",
    "running_correct = 0\n",
    "#running_loss_test = 0\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "steps_until_print = batch_size\n",
    "\n",
    "# stop_training = False\n",
    "# def sigint_handler(sig, frame):\n",
    "#      stop_training = True\n",
    "\n",
    "# signal.signal(signal.SIGINT, sigint_handler)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # if stop_training: break\n",
    "\n",
    "    # Check for stop - read file for boolean to stopping safely\n",
    "    with open(\"train.json\") as train_json:\n",
    "        train_dict = json.load(train_json)\n",
    "        if train_dict[\"train\"] == \"False\": break\n",
    "\n",
    "    testing_batches = iter(val_loader) # Every epoch tests the whole dataset once\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "            # Check for stop - read file for boolean to stopping safely\n",
    "        with open(\"train.json\") as train_json:\n",
    "            train_dict = json.load(train_json)\n",
    "            if train_dict[\"train\"] == \"False\": break\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print('images  shape:', images.shape)\n",
    "        # print('targets shape:', targets.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)['out']\n",
    "        # print(\"outputs shape:\", outputs.shape)\n",
    "      \n",
    "        loss = criterion(outputs, targets.long())\n",
    "\n",
    "        # Backward pass\n",
    "        optimiser.zero_grad()   # Clear old gradient values\n",
    "        loss.backward()         # Calculate the gradients\n",
    "        optimiser.step()        # Update the model's weights - seen at model.parameters()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if (i+1) % 1 == 0: # Maybe replace 1 with steps_until_print\n",
    "                # Logging the accuracy\n",
    "                pred = torch.argmax(outputs, dim=1)     # Evaluate along the 1st dimension\n",
    "                batch_pixel_accuracy = (pred == targets).sum().item()/(batch_size*pred.shape[1]*pred.shape[2])\n",
    "                writer.add_scalar('Accuracy/training', batch_pixel_accuracy, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step \n",
    "\n",
    "                # Logging the loss\n",
    "                writer.add_scalar('Loss/training', loss.item()/steps_until_print, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step\n",
    "\n",
    "            # If for every 5 batches, test one batch. (test:train data ratio is split 1:5)\n",
    "            if (i+1) % 5 == 0:  # Logging the testing loss\n",
    "                test_images, test_targets = testing_batches.next()\n",
    "                \n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device).squeeze(1)\n",
    "\n",
    "                model.eval()\n",
    "                test_outputs = model(test_images)['out']\n",
    "                model.train()\n",
    "                    \n",
    "                test_pred = torch.argmax(test_outputs, dim=1)\n",
    "\n",
    "                '''Plot test results'''\n",
    "                # fig, ax = plt.subplots(ncols=3, figsize=(24, 16))\n",
    "                # ax[0].imshow(test_images[0].to('cpu').squeeze().permute(1,2,0))  # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "                # ax[1].imshow(test_targets[0].to('cpu').squeeze()) # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "                # ax[2].imshow(test_pred[0].cpu().detach())\n",
    "                # plt.pause(0.01)\n",
    "                '''                     '''\n",
    "\n",
    "                # print('test_images  shape:', test_images.shape)\n",
    "                # print('test_targets shape:', test_targets.shape)\n",
    "                # print('test_outputs shape:', test_outputs.shape)\n",
    "                # print('test_pred    shape:', test_pred.shape)\n",
    "\n",
    "                # writer.add_images('test/images',      test_images                  , epoch * n_total_steps + i)\n",
    "                # writer.add_images('test/targets',     test_targets.unsqueeze(dim=1), epoch * n_total_steps + i)\n",
    "                # writer.add_images('test/predictions', test_pred.unsqueeze(dim=1)   , epoch * n_total_steps + i)\n",
    "\n",
    "                test_batch_pixel_accuracy = (test_pred == test_targets).sum().item()/(batch_size*test_pred.shape[1]*test_pred.shape[2])\n",
    "                #print('Test batch pixel accuracy', test_batch_pixel_accuracy)\n",
    "                writer.add_scalar('Accuracy/testing', test_batch_pixel_accuracy, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step \n",
    "\n",
    "                test_loss = criterion(test_outputs, test_targets.long())\n",
    "                writer.add_scalar('Loss/testing', test_loss.item()/len(test_targets), epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
    "                \n",
    "\n",
    "    #print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
    "\n",
    "print(\"Training is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Saving the model''';\n",
    "# checkpoint = {\n",
    "#     \"epoch\": epoch,\n",
    "#     \"model_state\": model.state_dict(),\n",
    "#     \"optimiser_state\": optimiser.state_dict()\n",
    "# }\n",
    "# torch.save(checkpoint, \"checkpoint_resnet50_0.5epoch_v2_cityscapes.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting''';\n",
    "with torch.no_grad():\n",
    "    iterator = iter(val_loader)\n",
    "    images, targets = next(iterator)\n",
    "    images = images.to(device)\n",
    "\n",
    "    model.eval().to(device)\n",
    "    output = model(images.to(device))['out']\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "\n",
    "    images = images.to('cpu')\n",
    "    targets = targets.to('cpu')\n",
    "    output = output.to('cpu')\n",
    "    pred = pred.to('cpu')\n",
    "    print('image: ', images.shape)\n",
    "    print('target:', targets.shape)\n",
    "    print('output:', output.shape)\n",
    "    print('pred:  ', pred.shape)\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(24, 16))\n",
    "    ax[0].imshow(images[1].squeeze().permute(1,2,0))  # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "    ax[1].imshow(targets[1].squeeze()) # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "    ax[2].imshow(pred[1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
