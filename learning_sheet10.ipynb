{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cityscapes - Resnet50 - to .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "#import torchinfo\n",
    "import json\n",
    "from torchvision.models.segmentation import fcn\n",
    "from matplotlib import cm\n",
    "import signal\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device chosen GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Device chosen GPU:\", torch.cuda.get_device_name(device))\n",
    "\n",
    "# Parameters\n",
    "num_classes = 35\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 12\n",
    "batch_size = 2\n",
    "learning_rate = 0.0001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "log_directory = f\"runs/Cityscapes/resnet50/v0.1.4 Adam lr = {learning_rate}, epochs = {num_epochs}, batchsize ={batch_size}\"\n",
    "writer  = SummaryWriter(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def pil_to_np(img):\n",
    "    return np.array(img) \n",
    "\n",
    "# The Cityscapes dataset is avaliable in PyTorch\n",
    "train_dataset = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='train', mode='fine', target_type='semantic', transform=transform, target_transform=pil_to_np)\n",
    "#test_dataset  = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='test',  mode='fine', target_type='semantic', transform=pil_to_tensor, target_transform=transforms.ToTensor())\n",
    "val_dataset   = torchvision.datasets.Cityscapes(root='./cityscapesDataset', split='val',   mode='fine', target_type='semantic', transform=transform, target_transform=pil_to_np)\n",
    "\n",
    "# Splitting the training and testing datasets into smaller batches\n",
    "workers = 5\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)#,  num_workers=workers)#, pin_memory=True))\n",
    "#test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False)#, num_workers=workers)#, pin_memory=True))\n",
    "val_loader   = torch.utils.data.DataLoader(dataset=val_dataset,  batch_size=batch_size, shuffle=False)#, num_workers=workers)#, pin_memory=True))\n",
    "\n",
    "#print('Train Size: ', len(train_dataset))\n",
    "#print('Test Size : ', len(test_dataset))\n",
    "#print('Val Size  : ', len(val_dataset))\n",
    "\n",
    "'''Plot from dataset'''\n",
    "# fig, ax = plt.subplots(ncols=2, figsize=(24, 16))\n",
    "# ax[0].imshow(np.array(train_dataset[1][0]).transpose(1,2,0)) # transpose(1,2,0) changes the order of the dimensions\n",
    "# ax[1].imshow(np.array(train_dataset[1][1]))\n",
    "\n",
    "'''Plot from dataloader''';\n",
    "# plot_img, plot_target = next(iter(val_loader))\n",
    "# plot_img = plot_img[0]\n",
    "# plot_target = plot_target[0]\n",
    "# print(plot_img.shape)\n",
    "# print(plot_target.shape)\n",
    "\n",
    "# fig, ax = plt.subplots(ncols=2, figsize=(24, 16))\n",
    "# ax[0].imshow(np.array(plot_img).transpose(1,2,0)) # transpose(1,2,0) changes the order of the dimensions\n",
    "# ax[1].imshow(np.array(plot_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading saved model''';\n",
    "model = fcn.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=False, pretrained_backbone=True).to(device)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=[beta1, beta2], eps=1e-08)\n",
    "\n",
    "loaded_checkpoint = torch.load(\"checkpoint_resnet50_0.5epoch_cityscapes.pth\")\n",
    "\n",
    "# for param in model.parameters():    # Freezing the startign layers\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model.load_state_dict(loaded_checkpoint[\"model_state\"])\n",
    "optimiser.load_state_dict(loaded_checkpoint[\"optimiser_state\"])\n",
    "epoch = loaded_checkpoint[\"epoch\"]\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  2975\n",
      "Val Size  :  500\n"
     ]
    }
   ],
   "source": [
    "'''Loading new model''';\n",
    "model = fcn.fcn_resnet50(pretrained=False, progress=True, num_classes=num_classes, aux_loss=False, pretrained_backbone=True).to(device)\n",
    "\n",
    "# Finetuning\n",
    "# for param in model.parameters():    # Freezing/unfreezing the starting layers\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=[beta1, beta2], eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training'''\n",
    "# Tensorboard\n",
    "#writer.add_graph(model.cpu(), val_dataset[0][0])\n",
    "#writer.close()\n",
    "\n",
    "# Doing the training now\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "steps_until_print = batch_size\n",
    "\n",
    "# stop_training = False\n",
    "# def signal_handler(sig, frame):\n",
    "#     print('\\nDetected Ctrl+C, stopping training')\n",
    "#     stop_training = True\n",
    "#     print('Saving model')\n",
    "# signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "model.train()\n",
    "print('Starting training')\n",
    "for epoch in range(num_epochs):\n",
    "    #if stop_training: break\n",
    "\n",
    "    # Check for stop - read file for boolean to stopping safely\n",
    "    with open(\"train.json\") as train_json:\n",
    "        train_dict = json.load(train_json)\n",
    "        if train_dict[\"train\"] == \"False\": break\n",
    "\n",
    "    testing_batches = iter(val_loader) # Every epoch tests the whole dataset once\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "            # Check for stop - read file for boolean to stopping safely\n",
    "        with open(\"train.json\") as train_json:\n",
    "            train_dict = json.load(train_json)\n",
    "            if train_dict[\"train\"] == \"False\": break\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print('images  shape:', images.shape)\n",
    "        # print('targets shape:', targets.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)['out']\n",
    "        # print(\"outputs shape:\", outputs.shape)\n",
    "      \n",
    "        loss = criterion(outputs, targets.long())\n",
    "\n",
    "        # Backward pass\n",
    "        optimiser.zero_grad()   # Clear old gradient values\n",
    "        loss.backward()         # Calculate the gradients\n",
    "        optimiser.step()        # Update the model's weights - seen at model.parameters()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Logging the train accuracy\n",
    "            pred = torch.argmax(outputs, dim=1)     # Evaluate along the 1st dimension\n",
    "            batch_pixel_accuracy = (pred == targets).sum().item()/(batch_size*pred.shape[1]*pred.shape[2])\n",
    "            writer.add_scalar('Accuracy/training', batch_pixel_accuracy, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step \n",
    "\n",
    "            # Logging the train loss\n",
    "            writer.add_scalar('Loss/training', loss.item()/steps_until_print, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step\n",
    "\n",
    "            # For every 5 batches, test one batch. (test:train data ratio is split 1:5)\n",
    "            if (i+1) % 5 == 0:  # Logging the testing loss\n",
    "                test_images, test_targets = testing_batches.next()\n",
    "                \n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device).squeeze(1)\n",
    "\n",
    "                model.eval()\n",
    "                test_outputs = model(test_images)['out']\n",
    "                model.train()\n",
    "                    \n",
    "                test_pred = torch.argmax(test_outputs, dim=1)\n",
    "\n",
    "                '''Plot test results'''\n",
    "                # fig, ax = plt.subplots(ncols=3, figsize=(24, 16))\n",
    "                # ax[0].imshow(test_images[0].to('cpu').squeeze().permute(1,2,0))  # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "                # ax[1].imshow(test_targets[0].to('cpu').squeeze()) # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "                # ax[2].imshow(test_pred[0].cpu().detach())\n",
    "                # plt.pause(0.01)\n",
    "                '''                     '''\n",
    "\n",
    "                # print('test_images  shape:', test_images.shape)\n",
    "                # print('test_targets shape:', test_targets.shape)\n",
    "                # print('test_outputs shape:', test_outputs.shape)\n",
    "                # print('test_pred    shape:', test_pred.shape)\n",
    "\n",
    "                # writer.add_images('test/images',      test_images                  , epoch * n_total_steps + i)\n",
    "                # writer.add_images('test/targets',     test_targets.unsqueeze(dim=1), epoch * n_total_steps + i)\n",
    "                # writer.add_images('test/predictions', test_pred.unsqueeze(dim=1)   , epoch * n_total_steps + i)\n",
    "\n",
    "                # Logging the test accuracy\n",
    "                test_batch_pixel_accuracy = (test_pred == test_targets).sum().item()/(batch_size*test_pred.shape[1]*test_pred.shape[2])\n",
    "                #print('Test batch pixel accuracy', test_batch_pixel_accuracy)\n",
    "                writer.add_scalar('Accuracy/testing', test_batch_pixel_accuracy, epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step \n",
    "\n",
    "                # Logging the test loss\n",
    "                test_loss = criterion(test_outputs, test_targets.long())\n",
    "                writer.add_scalar('Loss/testing', test_loss.item()/len(test_targets), epoch * n_total_steps + i) # label of the scalar, actual loss mean, current global step\n",
    "\n",
    "                #writer.add_scalars(\"Accuracy\", {\"train\": batch_pixel_accuracy, \"test\": test_batch_pixel_accuracy}, epoch * n_total_steps + i)\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
    "                \n",
    "\n",
    "    #print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
    "\n",
    "print(\"Training is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Saving the model''';\n",
    "# checkpoint = {\n",
    "#     \"epoch\": epoch,\n",
    "#     \"model_state\": model.state_dict(),\n",
    "#     \"optimiser_state\": optimiser.state_dict()\n",
    "# }\n",
    "# torch.save(checkpoint, \"checkpoint_resnet50_0.5epoch_v2_cityscapes.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting''';\n",
    "with torch.no_grad():\n",
    "    iterator = iter(val_loader)\n",
    "    images, targets = next(iterator)\n",
    "    images = images.to(device)\n",
    "\n",
    "    model.eval().to(device)\n",
    "    output = model(images.to(device))['out']\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "\n",
    "    images = images.to('cpu')\n",
    "    targets = targets.to('cpu')\n",
    "    output = output.to('cpu')\n",
    "    pred = pred.to('cpu')\n",
    "    print('image: ', images.shape)\n",
    "    print('target:', targets.shape)\n",
    "    print('output:', output.shape)\n",
    "    print('pred:  ', pred.shape)\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(24, 16))\n",
    "    ax[0].imshow(images[1].squeeze().permute(1,2,0))  # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "    ax[1].imshow(targets[1].squeeze()) # .squeeze() does the same thing as .numpy().transpose(1,2,0) \n",
    "    ax[2].imshow(pred[1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
